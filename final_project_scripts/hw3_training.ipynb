{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00268fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import jsonlines\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BlenderbotForConditionalGeneration,\n",
    "    BlenderbotTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b456eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './extract.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db18c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with jsonlines.open(train_path) as f:\n",
    "    for line in f.iter():\n",
    "        train_data.append(line)\n",
    "# val_data = []\n",
    "# with jsonlines.open(val_path) as f:\n",
    "#     for line in f.iter():\n",
    "#         val_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73dae39c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname).to(device)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916d779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    encode_examples = {}\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for doc in examples:\n",
    "        for i, dialog in enumerate(doc['dialog']):\n",
    "            if i%2 == 1:\n",
    "                inputs.append(dialog)\n",
    "            else:\n",
    "                targets.append(dialog)\n",
    "    # inputs = [doc[\"maintext\"] for doc in examples]\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encode_examples = tokenizer(inputs, max_length=100, truncation=True, padding=True)\n",
    "    encode_examples['target'] = tokenizer(targets, max_length=100, truncation=True, padding=True)\n",
    "    \n",
    "    return encode_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cff6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summerize = {}\n",
    "tokenized_set = preprocess_function(train_data)\n",
    "#tokenized_summerize['validation'] = preprocess_function(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50de08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98406ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialog(Dataset):\n",
    "    def __init__(self, encoded_dataset):\n",
    "        self.token = encoded_dataset['input_ids']\n",
    "        self.label = encoded_dataset['target']['input_ids'] if 'target' in encoded_dataset.keys() else None\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is None:\n",
    "            return torch.tensor(self.token[index])\n",
    "        else:\n",
    "            return torch.tensor(self.token[index]), torch.tensor(self.label[index])\n",
    "    def __len__(self):\n",
    "        return len(self.token)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23615d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dialog(tokenized_set)\n",
    "#valset = Dialog(tokenized_summerize['validation'])\n",
    "trainloader = DataLoader(dataset = trainset, batch_size = 64, shuffle = True)\n",
    "#valloader = DataLoader(dataset = valset, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36364911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    #torch.save(model.state_dict(), checkpoint_path)\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    tokenizer.save_pretrained(checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    model = torch.load(checkpoint_path)\n",
    "    #model.load_state_dict(state['state_dict'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63ac116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge_list = []\n",
    "# def test(model):\n",
    "#     model.eval()\n",
    "#     label_list,eval_predictions = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for i,(inputs_ids, labels) in enumerate(valloader):\n",
    "#             if i%170 == 0 and i != 0:\n",
    "#                 print(f\"---start evaluation---\")\n",
    "#             inputs_ids, labels = inputs_ids.to(device), labels.to(device)\n",
    "#             outputs = model.generate(inputs_ids,num_beams=5,max_length=64)\n",
    "#             eval_predictions  += outputs\n",
    "#             label_list += labels\n",
    "#         rouge = compute_metrics(eval_predictions, label_list)\n",
    "#         print(rouge)\n",
    "#         rouge_list.append(rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b912b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(model, epoch, save_interval, log_interval=100):\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=1e-4, betas=(0.9, 0.999),weight_decay=0)\n",
    "    model.train()  # set training mode\n",
    "    best = 0    \n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        loss_sum = 0\n",
    "        for batch_idx, (input_ids, labels) in enumerate(trainloader):\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids=input_ids,labels=labels)\n",
    "            loss = output.loss\n",
    "            loss_sum = loss_sum + loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(input_ids), len(trainloader.dataset),\n",
    "                    100. * batch_idx / len(trainloader), loss.item()))\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                # test(model)\n",
    "                if iteration>500:\n",
    "                    save_checkpoint('./summary-%i/' % iteration, model, optimizer)    \n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "        print('\\nTrain set:Loss: ({:.0f})\\n'.format(loss_sum / len(trainloader)))\n",
    "        \n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('./summary-%i/' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a342d3e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2880) to match target batch_size (2240).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5bcda3b5fc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a47e1abfd7ea>\u001b[0m in \u001b[0;36mtrain_save\u001b[0;34m(model, epoch, save_interval, log_interval)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2880) to match target batch_size (2240)."
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "train_save(model, epoch = 8, save_interval = 500, log_interval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64360c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fa0578c3d0ab8305ee21e0a4f0a06849b2568e8436229705a3cd15e2e123ffa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
